{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task description\n- Classify the speakers of given features.\n- Main goal: Learn how to use transformer.\n- Baselines:\n  - Easy: Run sample code and know how to use transformer.\n  - Medium: Know how to adjust parameters of transformer.\n  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n\n- Other links\n  - Competiton: [link](https://www.kaggle.com/t/49ea0c385a974db5919ec67299ba2e6b)\n  - Slide: [link](https://docs.google.com/presentation/d/1LDAW0GGrC9B6D7dlNdYzQL6D60-iKgFr/edit?usp=sharing&ouid=104280564485377739218&rtpof=true&sd=true)\n  - Data: [link](https://github.com/googly-mingto/ML2023HW4/releases)\n","metadata":{"id":"C_jdZ5vHJ4A9"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_seed(87)","metadata":{"id":"E6burzCXIyuA","execution":{"iopub.status.busy":"2023-04-05T08:14:12.588990Z","iopub.execute_input":"2023-04-05T08:14:12.589534Z","iopub.status.idle":"2023-04-05T08:14:12.598456Z","shell.execute_reply.started":"2023-04-05T08:14:12.589479Z","shell.execute_reply":"2023-04-05T08:14:12.597384Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\n## Dataset\n- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n- We randomly select 600 speakers from Voxceleb2.\n- Then preprocess the raw waveforms into mel-spectrograms.\n\n- Args:\n  - data_dir: The path to the data directory.\n  - metadata_path: The path to the metadata.\n  - segment_len: The length of audio segment for training. \n- The architecture of data directory \\\\\n  - data directory \\\\\n  |---- metadata.json \\\\\n  |---- testdata.json \\\\\n  |---- mapping.json \\\\\n  |---- uttr-{random string}.pt \\\\\n\n- The information in metadata\n  - \"n_mels\": The dimention of mel-spectrogram.\n  - \"speakers\": A dictionary. \n    - Key: speaker ids.\n    - value: \"feature_path\" and \"mel_len\"\n\n\nFor efficiency, we segment the mel-spectrograms into segments in the traing step.","metadata":{"id":"k7dVbxW2LASN"}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport random\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n \n \nclass myDataset(Dataset):\n    def __init__(self, data_dir, segment_len=128):\n        self.data_dir = data_dir\n        self.segment_len = segment_len\n\n        # Load the mapping from speaker neme to their corresponding id. \n        mapping_path = Path(data_dir) / \"mapping.json\"\n        mapping = json.load(mapping_path.open())\n        self.speaker2id = mapping[\"speaker2id\"]\n\n        # Load metadata of training data.\n        metadata_path = Path(data_dir) / \"metadata.json\"\n        metadata = json.load(open(metadata_path))[\"speakers\"]\n\n        # Get the total number of speaker.\n        self.speaker_num = len(metadata.keys())\n        self.data = []\n        for speaker in metadata.keys():\n            for utterances in metadata[speaker]:\n                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n\n    def __len__(self):\n            return len(self.data)\n\n    def __getitem__(self, index):\n        feat_path, speaker = self.data[index]\n        # Load preprocessed mel-spectrogram.\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n\n        # Segmemt mel-spectrogram into \"segment_len\" frames.\n        if len(mel) > self.segment_len:\n            # Randomly get the starting point of the segment.\n            start = random.randint(0, len(mel) - self.segment_len)\n            # Get a segment with \"segment_len\" frames.\n            mel = torch.FloatTensor(mel[start:start+self.segment_len])\n        else:\n            mel = torch.FloatTensor(mel)\n        # Turn the speaker id into long for computing loss later.\n        speaker = torch.FloatTensor([speaker]).long()\n        return mel, speaker\n\n    def get_speaker_number(self):\n        return self.speaker_num","metadata":{"id":"KpuGxl4CI2pr","execution":{"iopub.status.busy":"2023-04-05T08:14:12.600468Z","iopub.execute_input":"2023-04-05T08:14:12.601095Z","iopub.status.idle":"2023-04-05T08:14:12.614155Z","shell.execute_reply.started":"2023-04-05T08:14:12.601060Z","shell.execute_reply":"2023-04-05T08:14:12.613121Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader\n- Split dataset into training dataset(90%) and validation dataset(10%).\n- Create dataloader to iterate the data.","metadata":{"id":"668hverTMlGN"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef collate_batch(batch):\n    # Process features within a batch.\n    \"\"\"Collate a batch of data.\"\"\"\n    mel, speaker = zip(*batch)\n    # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n    mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n    # mel: (batch size, length, 40)\n    return mel, torch.FloatTensor(speaker).long()\n\n\ndef get_dataloader(data_dir, batch_size, n_workers):\n    \"\"\"Generate dataloader\"\"\"\n    dataset = myDataset(data_dir)\n    speaker_num = dataset.get_speaker_number()\n    # Split dataset into training dataset and validation dataset\n    trainlen = int(0.9 * len(dataset))\n    lengths = [trainlen, len(dataset) - trainlen]\n    trainset, validset = random_split(dataset, lengths)\n\n    train_loader = DataLoader(\n        trainset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=n_workers,\n        pin_memory=True,\n        collate_fn=collate_batch,\n    )\n    valid_loader = DataLoader(\n        validset,\n        batch_size=batch_size,\n        num_workers=n_workers,\n        drop_last=True,\n        pin_memory=True,\n        collate_fn=collate_batch,\n    )\n\n    return train_loader, valid_loader, speaker_num","metadata":{"id":"B7c2gZYoJDRS","execution":{"iopub.status.busy":"2023-04-05T08:14:12.615945Z","iopub.execute_input":"2023-04-05T08:14:12.616804Z","iopub.status.idle":"2023-04-05T08:14:12.628908Z","shell.execute_reply.started":"2023-04-05T08:14:12.616769Z","shell.execute_reply":"2023-04-05T08:14:12.627707Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- TransformerEncoderLayer:\n  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n  - Parameters:\n    - d_model: the number of expected features of the input (required).\n\n    - nhead: the number of heads of the multiheadattention models (required).\n\n    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n\n    - dropout: the dropout value (default=0.1).\n\n    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n\n- TransformerEncoder:\n  - TransformerEncoder is a stack of N transformer encoder layers\n  - Parameters:\n    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n\n    - num_layers: the number of sub-encoder-layers in the encoder (required).\n\n    - norm: the layer normalization component (optional).","metadata":{"id":"5FOSZYxrMqhc"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n\n# Self-Attention Pooling 參考 : https://blog.csdn.net/YI_SHU_JIA/article/details/124423639\nclass Self_Attentive_Pooling(nn.Module):\n    def __init__(self,dim):\n        super(Self_Attentive_Pooling,self).__init__()\n        self.sap_linear=nn.Linear(dim,dim)\n        self.attention=nn.Parameter(torch.FloatTensor(dim,1))\n       \n    def forward(self,x):\n        x=x.permute(0,2,1)\n        h=torch.tanh(self.sap_linear(x))\n        w=torch.matmul(h,self.attention).squeeze(dim=2)\n        w=F.softmax(w,dim=1).view(x.size(0),x.size(1),1)\n        x=torch.sum(x*w,dim=1)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T08:14:12.631528Z","iopub.execute_input":"2023-04-05T08:14:12.632116Z","iopub.status.idle":"2023-04-05T08:14:12.642318Z","shell.execute_reply.started":"2023-04-05T08:14:12.632080Z","shell.execute_reply":"2023-04-05T08:14:12.641418Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n!pip install conformer #第一次執行程式先install #模型來源 : https://github.com/lucidrains/conformer\nfrom conformer import ConformerBlock\n\nclass Classifier(nn.Module):\n    def __init__(self, d_model=512, n_spks=600, dropout=0.25):\n        super().__init__()\n        # Project the dimension of features from that of input into d_model.\n        self.prenet = nn.Linear(40, d_model)\n        # TODO:\n        #   Change Transformer to Conformer.\n        #   https://arxiv.org/abs/2005.08100\n        \n        # Conformer 模型參考 : https://blog.csdn.net/weixin_42369818/article/details/124034996\n        self.encoder_layer = ConformerBlock(\n            dim = d_model,\n            dim_head = 8,\n            heads = 8,\n            ff_mult = 4,\n            conv_expansion_factor = 2,\n            conv_kernel_size = 30,\n            attn_dropout = dropout,\n            ff_dropout = dropout,\n            conv_dropout = dropout,\n        )\n\n        # Project the the dimension of features from d_model into speaker nums.\n        self.pred_layer = nn.Sequential(\n            nn.BatchNorm1d(d_model),\n            nn.Linear(d_model, n_spks),\n        )\n        \n        self.pooling=Self_Attentive_Pooling(d_model)\n        \n\n    def forward(self, mels):\n        \"\"\"\n        args:\n            mels: (batch size, length, 40)\n        return:\n            out: (batch size, n_spks)\n        \"\"\"\n        # out: (batch size, length, d_model)\n        out = self.prenet(mels)\n        # out: (length, batch size, d_model)\n        out = out.permute(1, 0, 2)\n        # The encoder layer expect features in the shape of (length, batch size, d_model).\n        out = self.encoder_layer(out)  \n        # out: (batch size, length, d_model)\n#         out = out.transpose(0, 1)\n        out=out.permute(1,2,0)\n        # mean pooling\n        stats = self.pooling(out)\n\n        # out: (batch, n_spks)\n        out = self.pred_layer(stats)\n        return out","metadata":{"id":"iXZ5B0EKJGs8","execution":{"iopub.status.busy":"2023-04-05T08:14:12.643921Z","iopub.execute_input":"2023-04-05T08:14:12.644580Z","iopub.status.idle":"2023-04-05T08:14:24.109016Z","shell.execute_reply.started":"2023-04-05T08:14:12.644538Z","shell.execute_reply":"2023-04-05T08:14:24.107858Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Collecting conformer\n  Downloading conformer-0.2.5-py3-none-any.whl (4.1 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from conformer) (1.13.0)\nCollecting einops\n  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->conformer) (4.4.0)\nInstalling collected packages: einops, conformer\nSuccessfully installed conformer-0.2.5 einops-0.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Learning rate schedule\n- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n- The warmup schedule\n  - Set learning rate to 0 in the beginning.\n  - The learning rate increases linearly from 0 to initial learning rate during warmup period.","metadata":{"id":"W7yX8JinM5Ly"}},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef get_cosine_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n):\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n        The optimizer for which to schedule the learning rate.\n        num_warmup_steps (:obj:`int`):\n        The number of steps for the warmup phase.\n        num_training_steps (:obj:`int`):\n        The total number of training steps.\n        num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n        The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n        following a half-cosine).\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n        The index of the last epoch when resuming training.\n\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    def lr_lambda(current_step):\n        # Warmup\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        # decadence\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps)\n        )\n        return max(\n            0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n        )\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"id":"ykt0N1nVJJi2","execution":{"iopub.status.busy":"2023-04-05T08:14:24.110640Z","iopub.execute_input":"2023-04-05T08:14:24.111024Z","iopub.status.idle":"2023-04-05T08:14:24.119754Z","shell.execute_reply.started":"2023-04-05T08:14:24.110984Z","shell.execute_reply":"2023-04-05T08:14:24.118696Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Model Function\n- Model forward function.","metadata":{"id":"-LN2XkteM_uH"}},{"cell_type":"code","source":"import torch\n\n\ndef model_fn(batch, model, criterion, device):\n    \"\"\"Forward a batch through the model.\"\"\"\n\n    mels, labels = batch\n    mels = mels.to(device)\n    labels = labels.to(device)\n\n    outs = model(mels)\n\n    loss = criterion(outs, labels)\n\n    # Get the speaker id with highest probability.\n    preds = outs.argmax(1)\n    # Compute accuracy.\n    accuracy = torch.mean((preds == labels).float())\n\n    return loss, accuracy","metadata":{"id":"N-rr8529JMz0","execution":{"iopub.status.busy":"2023-04-05T08:14:24.122718Z","iopub.execute_input":"2023-04-05T08:14:24.123431Z","iopub.status.idle":"2023-04-05T08:14:24.136361Z","shell.execute_reply.started":"2023-04-05T08:14:24.123390Z","shell.execute_reply":"2023-04-05T08:14:24.135290Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Validate\n- Calculate accuracy of the validation set.","metadata":{"id":"cwM_xyOtNCI2"}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\n\ndef valid(dataloader, model, criterion, device): \n    \"\"\"Validate on validation set.\"\"\"\n\n    model.eval()\n    running_loss = 0.0\n    running_accuracy = 0.0\n    pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n\n    for i, batch in enumerate(dataloader):\n        with torch.no_grad():\n            loss, accuracy = model_fn(batch, model, criterion, device)\n            running_loss += loss.item()\n            running_accuracy += accuracy.item()\n\n        pbar.update(dataloader.batch_size)\n        pbar.set_postfix(\n            loss=f\"{running_loss / (i+1):.2f}\",\n            accuracy=f\"{running_accuracy / (i+1):.2f}\",\n        )\n\n    pbar.close()\n    model.train()\n\n    return running_accuracy / len(dataloader)","metadata":{"id":"YAiv6kpdJRTJ","execution":{"iopub.status.busy":"2023-04-05T08:14:24.138031Z","iopub.execute_input":"2023-04-05T08:14:24.138437Z","iopub.status.idle":"2023-04-05T08:14:24.150749Z","shell.execute_reply.started":"2023-04-05T08:14:24.138402Z","shell.execute_reply":"2023-04-05T08:14:24.149642Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Main function","metadata":{"id":"g6ne9G-eNEdG"}},{"cell_type":"code","source":"from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, random_split\n\n\ndef parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"/kaggle/input/ml2023springhw4/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 64,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 200000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    model = Classifier(n_spks=speaker_num).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = AdamW(model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, model, criterion, device)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"id":"Usv9s-CuJSG7","outputId":"f4f6a983-3559-4f36-efae-402bbf790473","execution":{"iopub.status.busy":"2023-04-05T08:14:24.153221Z","iopub.execute_input":"2023-04-05T08:14:24.154041Z","iopub.status.idle":"2023-04-05T08:50:31.895921Z","shell.execute_reply.started":"2023-04-05T08:14:24.154003Z","shell.execute_reply":"2023-04-05T08:50:31.894254Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [02:35<00:00, 12.89 step/s, accuracy=0.00, loss=nan, step=2000]\nValid:  99% 5632/5667 [00:05<00:00, 1076.47 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:28<00:00, 13.45 step/s, accuracy=0.00, loss=nan, step=4000]\nValid:  99% 5632/5667 [00:03<00:00, 1618.11 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:27<00:00, 13.53 step/s, accuracy=0.00, loss=nan, step=6000]\nValid:  99% 5632/5667 [00:04<00:00, 1394.98 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:28<00:00, 13.50 step/s, accuracy=0.00, loss=nan, step=8000]\nValid:  99% 5632/5667 [00:03<00:00, 1610.73 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:27<00:00, 13.56 step/s, accuracy=0.02, loss=nan, step=1e+4]\nValid:  99% 5632/5667 [00:03<00:00, 1575.97 uttr/s, accuracy=0.00, loss=nan]\nTrain:   0% 2/2000 [00:00<03:46,  8.83 step/s, accuracy=0.00, loss=nan, step=1e+4]","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.0005)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [02:28<00:00, 13.48 step/s, accuracy=0.00, loss=nan, step=12000]\nValid:  99% 5632/5667 [00:03<00:00, 1529.81 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:27<00:00, 13.52 step/s, accuracy=0.00, loss=nan, step=14000]\nValid:  99% 5632/5667 [00:03<00:00, 1665.88 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:28<00:00, 13.51 step/s, accuracy=0.00, loss=nan, step=16000]\nValid:  99% 5632/5667 [00:03<00:00, 1610.83 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:27<00:00, 13.53 step/s, accuracy=0.00, loss=nan, step=18000]\nValid:  99% 5632/5667 [00:03<00:00, 1662.92 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:28<00:00, 13.50 step/s, accuracy=0.00, loss=nan, step=2e+4] \nValid:  99% 5632/5667 [00:03<00:00, 1659.22 uttr/s, accuracy=0.00, loss=nan]\nTrain:   0% 2/2000 [00:00<04:20,  7.68 step/s, accuracy=0.00, loss=nan, step=2e+4]","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.0005)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [02:30<00:00, 13.31 step/s, accuracy=0.00, loss=nan, step=22000]\nValid:  99% 5632/5667 [00:04<00:00, 1162.09 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:29<00:00, 13.37 step/s, accuracy=0.00, loss=nan, step=24000]\nValid:  99% 5632/5667 [00:03<00:00, 1625.88 uttr/s, accuracy=0.00, loss=nan]\nTrain: 100% 2000/2000 [02:27<00:00, 13.56 step/s, accuracy=0.00, loss=nan, step=26000]\nValid:  99% 5632/5667 [00:03<00:00, 1680.08 uttr/s, accuracy=0.00, loss=nan]\nTrain:  96% 1912/2000 [02:22<00:06, 13.78 step/s, accuracy=0.00, loss=nan, step=27912]","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2348026573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_24/2348026573.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir, save_path, batch_size, n_workers, valid_steps, warmup_steps, total_steps, save_steps)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mbatch_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/2784637243.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(batch, model, criterion, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/2184647194.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mels)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# The encoder layer expect features in the shape of (length, batch size, d_model).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;31m# out: (batch size, length, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#         out = out.transpose(0, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/conformer/conformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/conformer/conformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/conformer/conformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context, mask, context_mask)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b n (h d) -> b h n d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mdots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b h i d, b h j d -> b h i j'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# shaw's relative positional embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Inference\n\n## Dataset of inference","metadata":{"id":"NLatBYAhNNMx"}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\n\n\nclass InferenceDataset(Dataset):\n    def __init__(self, data_dir):\n        testdata_path = Path(data_dir) / \"testdata.json\"\n        metadata = json.load(testdata_path.open())\n        self.data_dir = data_dir\n        self.data = metadata[\"utterances\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        utterance = self.data[index]\n        feat_path = utterance[\"feature_path\"]\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n\n        return feat_path, mel\n\n\ndef inference_collate_batch(batch):\n    \"\"\"Collate a batch of data.\"\"\"\n    feat_paths, mels = zip(*batch)\n\n    return feat_paths, torch.stack(mels)","metadata":{"id":"efS4pCmAJXJH","execution":{"iopub.status.busy":"2023-04-05T08:50:36.780560Z","iopub.execute_input":"2023-04-05T08:50:36.781307Z","iopub.status.idle":"2023-04-05T08:50:36.790180Z","shell.execute_reply.started":"2023-04-05T08:50:36.781261Z","shell.execute_reply":"2023-04-05T08:50:36.788994Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Main funcrion of Inference","metadata":{"id":"tl0WnYwxNK_S"}},{"cell_type":"code","source":"import json\nimport csv\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"/kaggle/input/ml2023springhw4/Dataset\",\n        \"model_path\": \"/kaggle/working/model.ckpt\",\n        \"output_path\": \"./output.csv\",\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    model_path,\n    output_path,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    mapping_path = Path(data_dir) / \"mapping.json\"\n    mapping = json.load(mapping_path.open())\n\n    dataset = InferenceDataset(data_dir)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=1,\n        shuffle=False,\n        drop_last=False,\n        num_workers=8,\n        collate_fn=inference_collate_batch,\n    )\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    speaker_num = len(mapping[\"id2speaker\"])\n    model = Classifier(n_spks=speaker_num).to(device)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    results = [[\"Id\", \"Category\"]]\n    for feat_paths, mels in tqdm(dataloader):\n        with torch.no_grad():\n            mels = mels.to(device)\n            outs = model(mels)\n            preds = outs.argmax(1).cpu().numpy()\n            for feat_path, pred in zip(feat_paths, preds):\n                results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n\n    with open(output_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerows(results)\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"id":"i8SAbuXEJb2A","outputId":"3808f409-19c9-426c-dc15-1b88b0c21645","execution":{"iopub.status.busy":"2023-04-05T08:51:17.944187Z","iopub.execute_input":"2023-04-05T08:51:17.944962Z","iopub.status.idle":"2023-04-05T08:52:18.749895Z","shell.execute_reply.started":"2023-04-05T08:51:17.944923Z","shell.execute_reply":"2023-04-05T08:52:18.747938Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddaebc52cc6046a5817df96361ad9fa2"}},"metadata":{}}]}]}